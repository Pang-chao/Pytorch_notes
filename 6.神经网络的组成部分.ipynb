{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1.层--神经网络的基本组成\n",
    "\n",
    "最重要的一种是线性层，如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.8152, -0.7249,  1.0178, -0.9301, -0.1114]],\n       grad_fn=<AddmmBackward>)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import Linear\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "myLayer = Linear(in_features=10, out_features=5, bias=True)\n",
    "inp = Variable(torch.randn(1,10))\n",
    "myLayer(inp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "可以使用属性weights和bias访问层的可训练参数。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([[-0.0588, -0.1034,  0.1875,  0.0318, -0.3108, -0.2996, -0.2199,  0.0197,\n         -0.1492, -0.0215],\n        [-0.0878, -0.2875,  0.1953, -0.2660,  0.2925, -0.1067, -0.2141, -0.2479,\n          0.2666,  0.0757],\n        [-0.1316, -0.1513,  0.1597, -0.2969, -0.1682,  0.0738, -0.2633, -0.0691,\n          0.2258, -0.2845],\n        [ 0.1739, -0.0897,  0.0947,  0.2147,  0.0351, -0.2237,  0.0652,  0.0528,\n         -0.0804, -0.1180],\n        [ 0.1792, -0.0443, -0.1721, -0.1628, -0.2420,  0.2688,  0.1696, -0.0314,\n          0.1419,  0.2344]], requires_grad=True)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myLayer.weight"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([-0.0976,  0.2117,  0.1996, -0.0337,  0.1045], requires_grad=True)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myLayer.bias"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "但一般不止一个层。\n",
    "可以通过把一层的输出传给另一层实现。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.5846,  0.4345]], grad_fn=<AddmmBackward>)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myLayerx = Linear(5,2)\n",
    "myLayerx(myLayer(inp))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "只堆叠多个线性层没有任何意义。\n",
    "可以使用不同的非线性函数。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.非线性激活函数\n",
    "\n",
    "（1）sigmoid\n",
    "（2）tanh\n",
    "（3）ReLU\n",
    "（4）Leaky ReLU"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 2., 0., 0.]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = Variable(torch.Tensor([[1,2,-1,-1]]))\n",
    "myrelu = nn.ReLU()\n",
    "myrelu(sample_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3.损失函数\n",
    "\n",
    "优化器通常接受标量。因而loss函数应该生成一个标量值，并使其在训练期间最小化。\n",
    "以MSE均方误差为例。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()\n",
    "input = Variable(torch.randn(3,5), requires_grad=True)\n",
    "target = Variable(torch.randn(3,5))\n",
    "output = loss(input, target)\n",
    "output.backward()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4.优化器\n",
    "\n",
    "以SGD为例"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr = 0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}