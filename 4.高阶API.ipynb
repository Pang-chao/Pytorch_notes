{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FizBuzNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(FizBuzNet, self).__init__()\n",
    "        self.hidden = nn.Linear(input_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        hidden = self.hidden(batch)\n",
    "        activated = torch.sigmoid(hidden)\n",
    "        out = self.out(activated)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "1.nn.Module\n",
    "\n",
    "可以将网络的每个可分离组件定义为继承自nn.Module的单独Python类。\n",
    "它实现了两个主要函数，__call__()与backward()，\n",
    "用户需要重写forward()和__init__()。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Linear类中，用Parameter封装张量。Parameter将权重和偏置添加到模块参数列表中，\n",
    "并且在调用model.Parameters()时可用。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "（1）apply\n",
    "\n",
    "apply(fn)：将fn函数递归地应用到网络模型的每个子模型中，主要用在参数的初始化。\n",
    "\n",
    "使用apply()时，需要先定义一个参数初始化的函数。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "    classname = m.__class__.__name__ # 得到网络层的名字，如ConvTranspose2d\n",
    "    if classname.find('Conv') != -1:  # 使用了find函数，如果不存在返回值为-1，所以让其不等于-1\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "之后，定义自己的网络，得到网络模型，使用apply()函数，就可以分别对conv层和bn层进行参数初始化。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = net()\n",
    "model.apply(weight_init)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "（2）train和eval\n",
    "\n",
    "model.train()和model.eval()的区别主要在于Batch Normalization和Dropout两层。\n",
    "\n",
    "如果模型中有BN层(Batch Normalization）和 Dropout，需要在训练时添加model.train()。\n",
    "model.train()是保证BN层能够用到每一批数据的均值和方差。\n",
    "对于Dropout，model.train()是随机取一部分网络连接来训练更新参数。\n",
    "\n",
    "如果模型中有BN层(Batch Normalization）和Dropout，在测试时添加model.eval()。\n",
    "model.eval()是保证BN层能够用全部训练数据的均值和方差，即测试过程中要保证BN层的均值和方差不变。\n",
    "对于Dropout，model.eval()是利用到了所有网络连接，即不进行随机舍弃神经元。\n",
    "\n",
    "训练完train样本后，生成的模型model要用来测试样本。\n",
    "在model(test)之前，需要加上model.eval()，否则的话，有输入数据，即使不训练，它也会改变权值。\n",
    "这是model中含有BN层和Dropout所带来的的性质。\n",
    "\n",
    "在做one classification的时候，训练集和测试集的样本分布是不一样的，尤其需要注意这一点。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "（3）parameters\n",
    "\n",
    "调用parameters会返回所有模型参数。对于优化器或使用参数进行试验非常重要。\n",
    "无需逐行写下每个参数更新，我们可以将其归纳为for循环。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net = FizBuzNet(input_size, hidden_size, output_size)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for p in net.parameters():\n",
    "        p -= p.grad * lr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "（4）zero_grad\n",
    "\n",
    "这是使梯度变为0的便捷函数。不必查找每个参数分别调用。\n",
    "对模型对象的一次调用将使得所有参数的梯度变成0。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.nn.function\n",
    "\n",
    "nn.functional是一个很常用的模块，nn中的大多数layer在functional中都有一个与之对应的函数。\n",
    "nn.functional中的函数与nn.Module()的区别是：\n",
    "\n",
    "nn.Module实现的层（layer）是一个特殊的类，都是由class Layer(nn.Module)定义，\n",
    "会自动提取可学习的参数。\n",
    "nn.functional中的函数更像是纯函数,由def functional(input）定义。\n",
    "\n",
    "注意：\n",
    "\n",
    "如果模型有可学习的参数时，最好使用nn.Module；\n",
    "否则既可以使用nn.functional也可以使用nn.Module，二者在性能上没有太大差异，\n",
    "具体的使用方式取决于个人喜好。\n",
    "由于激活函数（ReLu、sigmoid、Tanh）、池化（MaxPool）等层没有可学习的参数，\n",
    "可以使用对应的functional函数，而卷积、全连接等有可学习参数的网络建议使用nn.Module。\n",
    "\n",
    "注意：\n",
    "\n",
    "虽然dropout没有可学习参数，但建议还是使用nn.Dropout而不是nn.functional.dropout，\n",
    "因为dropout在训练和测试两个阶段的行为有所差别，\n",
    "使用nn.Module对象能够通过model.eval操作加以区分。\n",
    "\n",
    "在代码中，不具备可学习参数的层（激活层、池化层），将它们用函数代替，\n",
    "这样可以不用放置在构造函数__init__中。有可学习的模块，也可以用functional代替，\n",
    "只不过实现起来比较繁琐，需要手动定义参数parameter，如前面实现自定义的全连接层，\n",
    "就可以将weight和bias两个参数单独拿出来，在构造函数中初始化为parameter。"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}